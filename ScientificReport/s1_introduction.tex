\chapter{Einführung}
\label{ch:einfuehrung}

Das effektive Ausfiltern von unerwünschten E-Mail-Nachrichten (oder kurz Spam-Nachrichten, nachfolgend nur mehr kurz ,,Spam``) ist eine wichtige praktische Aufgabe von maschinellen Lernsystemen. Wichtig ist dabei besonders, die Anzahl der ,,False Positives`` möglichst gering zu halten, da eine legitime E-Mail-Nachricht, welche versehentlich als Spam klassifiziert wird, unangenehme Konsequenzen haben kann.

In unserer Arbeit vergleichen wir die Ergebnisse von drei verschiedenen Klassifikationssystemen: k-Nearest-Neighbor-Algorithmus (kNN), Perceptron und der Mahalanobis-Distanz sowohl hinsichtlich ihrer Ergebnisse als auch ihrer Laufzeit anhan der frei verfügbaren Spambase-Datenbank.

\section{Verwendete Klassifikatoren}

In diesem Abschnitt werden die von uns verwendeten Klassifikatoren kurz vorgestellt.

\subsection{k-Nearest-Neighbor}

Beim k-Nearest-Neighbor werden die Daten eines Trainings-Sets in einem $n$-dimensionalen Raum aufgeteilt, wobei $n$ die Anzahl der Features ist. Wird ein Element klassifiziert, dann wird es jener Kategorie zugeordet, welcher der Mehrheit der nähesten $k$ Elemente angehören.

\subsection{Perceptron}

Das Perceptron ist eine einfache Variante eines neuronalen Netzes. Das Prinzip wurde erstmals im Jahre 1958 von Frank Rosenblatt veröffentlicht\cite{rosenblatt58}. Es handelt sich dabei um eine lineare Diskriminantenfunktion. Während des Lernvorganges wird ein Vektor mit Gewichten erstellt, welcher dann anschließend eine Klassifikation vornimmt. Das Ergebnis wird anschließend durch eine Signum-Funktion dargstellt.

\subsection{Mahalanobis-Distanz}

Die Mahalanobis-Distanz ist ein Distanzmaß, welches nach Mahalanobis benannt ist und von diesem erstmals 1936 vorgestellt wurde \cite{mahalanobis1936}. Bei diesem Klassifikator wird ein Objekt als jenes Objekt klassifiziert, welches die geringste Mahalnobis-Distanz aufweist. Wir berechnen die Mahalanobis-Distanz mit folgender Formel: 

\begin{verbatim}
distances(i) = (objectvector - trainedSet{i, 1}) *
          inv(trainedSet{i, 2}) * (objectvector - trainedSet{i, 1})';
\end{verbatim}


\section{Vorgehensweise}
\label{sec:vorgehensweise}

Für den Vergleich der verschiedenen Spam-Klassifikatoren verwenden wir die ,,Spambase``-Datenbank welche wir vom UC Irvine Machine Learning Repository\footnote{\url{http://archive.ics.uci.edu/ml/}} heruntergeladen haben. Von diesen sind 1813 als Spam gekennzeichnet. Insgesamt haben wir von diesen 4601 Datensätzen 4140 ($\approx 90~\%$) für das Trainings-Set $n$ und 461 ($\approx 10~\%$) für das Test-Set $t$ verwendet. Jeder Test-Datensatz besitzt jeweils Informationen zu 58 Features $d$. Das Verhältnis $n$ zu $d$ liegt damit bei $71,38$. Vor der Verarbeitung der Daten werden diese noch auf das Traingins-Set normalisiert. Jede Spalte $t_i$ wird dabei durch $max(n_i)$ dividiert.

\section{Feature-Reduktion}
\label{sec:feature_red}
Die Anzahl der Features spielt eine wichtige Rolle für die Klassifizerung. Je weniger Features verwendet werden, desto weniger Zeit wird für die Klassifzierung benötigt (wie wir in Abschnitt \ref{subsect:feat_runtime_fx} noch zeigen werden). Um die Features reduzieren zu können, haben wir haben uns dazu entschieden, die folgenden zwei Vorgehensweisen näher zu untersuchen:

\begin{itemize}
	\item Entfernung von Features mit geringer Varianz.
	\item Schrittweise Entfernung des Features mit dem geringsten Effekt.
\end{itemize}

\subsection{Entfernung von Features mit geringer Varianz}

Die Idee hinter dieser Art der Dimensionsreduktion ist, jene Features zu entfernen, die eine geringe Varianz aufweisen und deshalb dazu führen, dass keine eindeutige Zuordnung möglich ist. Dazu wird erst für jede Feature-Spalte des Traings-Sets $n_i$ die Varianz gebildet. Anschließend wird eine Schranke $max_{var}$ definiert, über der die Varianz der Feature-Spalte liegen muss, um berücksichtigt zu werden.

\subsection{Schrittweise Entfernung des Features mit dem geringsten Effekt.}

Hier war folgender Gedanke ausschlaggebend: Ausgehend von den vorhandenen $n$ Features wird jenes ermittelt, welches den geringsten Effekt hat. Mit ,,geringster Effekt`` ist dabei jenes Feature gemeint, das den Wert für die \textit{false positives} am wenigsten negativ beeinflusst, da wir diesen Wert so gering wie möglich halten wollen. Das durch dieses Verfahren ausgewählte Feature wird anschließend entfernt und aus dem daraus resultierenden Set wird der Schritt dann so lange wiederholt, bis eine definierte Grenze an Iterationen erreicht wurde oder das Verfahren abgebrochen wird.
