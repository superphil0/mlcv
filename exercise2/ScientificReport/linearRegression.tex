\section{Linear Regression}

Implementing linear regression was the first part of the assignment.

\subsection{Introduction}

This assignment was seperated into three parts.
\begin{enumerate}
	\item Data Generation
	\item Implementation of the algorithm
	\item Evaluation of the result
\end{enumerate}

\subsection{Data Generation}

For the linear regression assignment we had specific description about how to generate the dataset. The dataset consists of $51$ points on the quadratic function $2*x^2 + G*x + 1$ where $G$ is the number of our group, which is $5$. For this dataset we also had to generate a trainingdataset, which is $1/6$ in size of the original dataset. Those points are taken from the original dataset and then shifted by a random number chosen by a normal distribution with parameter $\mu = 0$ and $\sigma = 0.6$.

\subsection{Implementation of the algorithm}

In this assignment we used the online learning rule of the gradient descent algorithm. Therefore, every trainingsexample changes the weights. In contrast by using the batch learning technique, we only update after the whole trainingset has been processed. 

\subsection{Evaluation of the Results} 


\subsection{Questions}

\subsubsection{Question 1: What weight vector was calculated using gradient descent?}

We got the weight vector $w = [0.4154, 5.2012, 2.0025]$ for the sample problem after 1000 iterations of gradient descent with the learning rate $\gamma = 0.001$.

\subsubsection{Question 2: How can you calculate the optimal weight directly? How much different is it to the result of gradient descent?}

We can use linear algebra to reformulate the gradient descent problem. After solving the formula we get $w = (XX^T)^{-1}Xt^T$. Therefore, if $XX^T$ is invertible we can use this formula to directly compute the optimal weight vector.

Our test data is randomly generated and the variance to the real data is also at random, therefore the weight vector had small differences in distances to each other. With this procedure we got t the optimal weight vector $w^* = [0.4040, 5.2352, 1.9910]$, whereby the gradient descent weight vector was $w = [0.4154, 5.2012, 2.0025]$. This result was obtained with the same testdata, we had used for question 1. 

\begin{figure}[!ht]
	\centering
	\includegraphics[width=0.5\textwidth]{img/gradientDescent}
	\caption{Performance by gradient descent on the real dataset}
	\label{fig:perGraDescent}
\end{figure}


\begin{figure}[!ht]
	\centering
	\includegraphics[width=0.5\textwidth]{img/optimalWeightVector}
	\caption{Performance by the optimal weight on the real dataset}
	\label{fig:perOptimal}
\end{figure}



\subsubsection{Question 3: }





